{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root added to PYTHONPATH: True\n",
      "added: A functional contextual, observer-centric, quantum mechanical, and neuro-symbolic approach to solving the alignment problem of artificial general intelligence: safe AI through intersecting computational psychological neuroscience and LLM architecture for emergent theory of mind\n",
      "added: Neuralizer: General Neuroimage Analysis without Re-Training\n",
      "added: Imaging Neuroscience opening editorial\n",
      "added: Decoding In-Context Learning: Neuroscience-inspired Analysis of Representations in Large Language Models\n",
      "added: Bayesian Tensor Factorized Mixed Eﬀects Vector Autoregressive Processes for Inferring Granger Causality Patterns from High-Dimensional Neuroimage Data\n",
      "added: Understanding Structure Of LLM using Neural Cluster Knockout\n",
      "added: Applications of generative adversarial networks in neuroimaging and clinical neuroscience\n",
      "added: Large language models surpass human experts in predicting neuroscience results\n",
      "added: The feasibility of artificial consciousness through the lens of neuroscience\n",
      "added: Convergence of Artificial Intelligence and Neuroscience towards the Diagnosis of Neurological Disorders—A Scoping Review\n",
      "added: Toward naturalistic neuroscience: Mechanisms underlying the flattening of brain hierarchy in movie-watching compared to rest and task\n",
      "added: Functional neuroimaging as a catalyst for integrated neuroscience\n",
      "added: Considering the Purposes of Moral Education with Evidence in Neuroscience: Emphasis on Habituation of Virtues and Cultivation of Phronesis\n",
      "added: Addressing Global Environmental Challenges to Mental Health Using Population Neuroscience: A Review.\n",
      "added: A longitudinal resource for population neuroscience of school-age children and adolescents in China\n",
      "added: Connecting Circuits with Networks in Addiction Neuroscience: A Salience Network Perspective\n",
      "added: Neuroscience-based psychotherapy: A position paper\n",
      "added: Suboptimal phenotypic reliability impedes reproducible human neuroscience\n",
      "added: Clinical Neuroscience Continuing Education for Psychiatrists\n",
      "added: Predictive Clinical Neuroscience Portal (PCNportal): instant online access to research-grade normative models for clinical neuroscientists.\n",
      "All papers have been added to the Notion database.\n"
     ]
    }
   ],
   "source": [
    "### 通过query搜索主题并把内容同步到notion database中\n",
    "\n",
    "## 生物\n",
    "# Define your Notion API key and database ID\n",
    "# NOTION_DATABASE_ID = \"3001b93dfe62442c90c96bf6fa888feb\" #生物\n",
    "# NOTION_DATABASE_ID = \"8655c5459f8740a28e9f26f2f45a87dd\" #agent\n",
    "# NOTION_DATABASE_ID = \"0dec242526284b4aad0de077ff4ccf30\" #quant\n",
    "# NOTION_DATABASE_ID = \"85f5f98d2b3b48b3a49215c16525c28c\" # 多模态大模型\n",
    "NOTION_DATABASE_ID = \"fff5a4c02cd580d491a9ef076c6509e3\" # neuroGPT\n",
    "\n",
    "# Define the search query\n",
    "query = \"neuroscience neuroimage llm\"\n",
    "\n",
    "import os,sys\n",
    "import requests\n",
    "\n",
    "# Get the project root directory\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Add the project root directory to the PYTHONPATH\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Verify if the project root directory is in PYTHONPATH\n",
    "print(\"Project root added to PYTHONPATH:\", project_root in sys.path)\n",
    "\n",
    "from open_science_agent.api.semantic_scholar_client import SemanticScholarClient\n",
    "from open_science_agent.api.notion_client import NotionClient\n",
    "from open_science_agent.data_processing.functions import map_paper_to_notion_properties\n",
    "\n",
    "# Initialize the SemanticScholarClient\n",
    "semantic_scholar_client = SemanticScholarClient(timeout=(30, 100))\n",
    "\n",
    "# Initialize the NotionClient\n",
    "notion_client = NotionClient()\n",
    "\n",
    "# Perform the search by relevance\n",
    "results = semantic_scholar_client.get_search_results_with_retries(query)\n",
    "\n",
    "# Iterate over the results and store them in the Notion database\n",
    "for paper in results.get(\"data\", []):\n",
    "    properties = map_paper_to_notion_properties(paper)\n",
    "    \n",
    "    filter_criteria = {\"property\": \"PaperId\",\"rich_text\": {\"equals\": paper.get('paperId')}}\n",
    "    query_results = notion_client.query_database(NOTION_DATABASE_ID, filter_criteria=filter_criteria)\n",
    "    if len(query_results['results'])==0:\n",
    "        feed_back = notion_client.create_page(database_id=NOTION_DATABASE_ID, properties=properties)\n",
    "        print('added:', paper.get(\"title\", {}))\n",
    "    else:\n",
    "        print('already added:', paper.get(\"title\", {}))\n",
    "\n",
    "print(\"All papers have been added to the Notion database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': 2,\n",
       " 'offset': 0,\n",
       " 'data': [{'paperId': '79198b4009b375ae3746b7331a57f2aef54a456c',\n",
       "   'externalIds': {'DOI': '10.1101/2022.08.04.502811', 'CorpusId': 251474099},\n",
       "   'corpusId': 251474099,\n",
       "   'publicationVenue': {'id': '027ffd21-ebb0-4af8-baf5-911124292fd0',\n",
       "    'name': 'bioRxiv',\n",
       "    'type': 'journal',\n",
       "    'url': 'http://biorxiv.org/'},\n",
       "   'url': 'https://www.semanticscholar.org/paper/79198b4009b375ae3746b7331a57f2aef54a456c',\n",
       "   'title': 'Uni-Fold: An Open-Source Platform for Developing Protein Folding Models beyond AlphaFold',\n",
       "   'abstract': 'Recent breakthroughs on protein structure prediction, namely AlphaFold, have led to unprecedented new possibilities in related areas. However, the lack of training utilities in its current open-source code hinders the community from further developing or adapting the model. Here we present Uni-Fold as a thoroughly open-source platform for developing protein folding models beyond AlphaFold. We reimplemented AlphaFold and AlphaFold-Multimer in the PyTorch framework, and reproduced their from-scratch training processes with equivalent or better accuracy. Based on various optimizations, Uni-Fold achieves about 2.2 times training acceleration compared with AlphaFold under similar hardware configuration. On a benchmark of recently released multimeric protein structures, Uni-Fold outperforms AlphaFold-Multimer by approximately 2% on the TM-Score. Uni-Fold is currently the only open-source repository that supports both training and inference of multimeric protein models. The source code, model parameters, test data, and web server of Uni-Fold are publicly available3.',\n",
       "   'venue': 'bioRxiv',\n",
       "   'year': 2022,\n",
       "   'referenceCount': 23,\n",
       "   'citationCount': 35,\n",
       "   'influentialCitationCount': 2,\n",
       "   'isOpenAccess': True,\n",
       "   'openAccessPdf': {'url': 'https://www.biorxiv.org/content/biorxiv/early/2022/08/30/2022.08.04.502811.full.pdf',\n",
       "    'status': 'GREEN'},\n",
       "   'fieldsOfStudy': ['Biology'],\n",
       "   's2FieldsOfStudy': [{'category': 'Biology', 'source': 'external'},\n",
       "    {'category': 'Computer Science', 'source': 's2-fos-model'},\n",
       "    {'category': 'Biology', 'source': 's2-fos-model'}],\n",
       "   'tldr': {'model': 'tldr@v2.0.0',\n",
       "    'text': 'This work reimplementedAlphaFold and AlphaFold-Multimer in the PyTorch framework, and reproduced their from-scratch training processes with equivalent or better accuracy, and presented Uni-Fold as a thoroughly open-source platform for developing protein folding models beyond AlphaFolds.'},\n",
       "   'publicationTypes': None,\n",
       "   'publicationDate': '2022-08-30',\n",
       "   'journal': {'name': 'bioRxiv'},\n",
       "   'citationStyles': {'bibtex': '@Article{Li2022UniFoldAO,\\n author = {Ziyao Li and Xuyang Liu and Weijie Chen and Fan Shen and Hangrui Bi and Guolin Ke and Linfeng Zhang},\\n booktitle = {bioRxiv},\\n journal = {bioRxiv},\\n title = {Uni-Fold: An Open-Source Platform for Developing Protein Folding Models beyond AlphaFold},\\n year = {2022}\\n}\\n'},\n",
       "   'authors': [{'authorId': '48458595', 'name': 'Ziyao Li'},\n",
       "    {'authorId': '2181111972', 'name': 'Xuyang Liu'},\n",
       "    {'authorId': '2181203771', 'name': 'Weijie Chen'},\n",
       "    {'authorId': '2181218369', 'name': 'Fan Shen'},\n",
       "    {'authorId': '2040280585', 'name': 'Hangrui Bi'},\n",
       "    {'authorId': '35286545', 'name': 'Guolin Ke'},\n",
       "    {'authorId': '2181109681', 'name': 'Linfeng Zhang'}]},\n",
       "  {'paperId': 'a50c2bfe968f21ba159e21ea113d68c092a747de',\n",
       "   'externalIds': {'CorpusId': 253369189},\n",
       "   'corpusId': 253369189,\n",
       "   'publicationVenue': None,\n",
       "   'url': 'https://www.semanticscholar.org/paper/a50c2bfe968f21ba159e21ea113d68c092a747de',\n",
       "   'title': 'Analytical Methods, and and wastewater data, recombinant analysis from SARS-CoV-2sequencing efforts.',\n",
       "   'abstract': 'The lack of preparedness for detecting the highly infectious SARS-CoV-2 pathogen — the pathogen responsible for the COVID-19 disease — caused enormous harm to the public health, the economy and society as a whole. It took ~60 days for the first RT-PCR tests for SARS-CoV-2 infection developed by the United States (US) Centers for Disease Control (CDC) to be made available. It then took >270 days to deploy 800,000 of these tests at a time when the estimated actual testing needs required over 6 million tests per day. Testing was therefore limited to only individuals with symptoms or individuals in close contact with confirmed positive cases. Testing strategies that can be deployed on a population scale at ‘day zero’ - i.e., at the time of the first reported case - are needed. Next Generation Sequencing (NGS) has day zero capabilities with the potential to enable feasible and very broad large-scale testing strategies. We show that the detection sensitivity of SARS-CoV-2 via NGS is equivalent to RT-PCR detection if relevant samples are depleted of abundant sequences that don’t contribute to pathogen detection or host response. In addition, we show that the proposed strategy can also be used for variant strain typing, co-infection detection, and individual human host response assessment - all in a single workflow using existing open-source analysis pipelines. The NGS framework we describe is pathogen agnostic, and therefore has the potential to radically transform how both very large-scale pandemic response and focused clinical infectious disease testing are pursued in the future. ---PAGEBREAK---Control genomes, and genome Abstract During the current outbreak, genomics has become an essential toolfor surveilling infectious disease outbreaks. A wide range of strategies andtechniques for sequencing and processing SARS-CoV-2 genomes are being used bylaboratories worldwide. These methods are quite different, and especially forcomputational processing, sometimes ad hoc. A standardized, well tested, andaccessible tool for consensus genome sequence determination, particularly foroutbreaks such as the ongoing COVID-19 pandemic, is critical to provide a solidgenomic basis for epidemiological analyses and well-informed decision making.Additionally, a tool that goes beyond consensus genome generation and canidentify recombinants and mixed infection is warranted based on currentevolutionary track of the virus. Here, we have developed a bioinformaticworkflow called EDGE COVID-19 (EC-19) that is capable of generating consensusgenomes of SARS-CoV-2 and further perform pertinent downstream analyses. EC-19 accommodates sequencing data generated with either the Illumina or OxfordNanopore or Pacbio platforms. Furthermore, using an intuitive web-basedinterface, this workflow automates SARS-CoV-2 reference-based genomeassembly, variant calling, lineage determination, wastewater sample lineageabundance estimation, genome recombinant analysis and provides the ability tosubmit the consensus sequence and necessary metadata to GenBank, GISAID, andINSDC raw data repositories. Abstract Background Despite efforts to standardize genomic outbreak analyses, both single nucleotide variant (SNV) and gene-by-gene (eg. cgMLST/wgMLST) approaches continue to be used. Multiple comparisons have demonstrated these approaches perform largely equivalently; however, SNV approaches are limited by challenges of data sharing and genetic recombination adjustment, while gene-by-gene approaches are limited by the need for a stable, curated, centrally-hosted scheme, limiting their utility across all bacteria. Methods We present refMLST, a tool for reference-based multilocus sequence typing of bacteria. refMLST functions by building a scheme from a single, “reference” genome annotation, and therefore functions across all bacterial species with a genome sequence available. We use NCBI’s reference genome annotation for each species, but any GenBank formatted file may be used. refMLST excludes mobile elements such as plasmids from the scheme. Similar to gene-by-gene analysis, scheme genes are then located in the query genome. Instead of a comparison to a pre-computed scheme to identify allele names, allele sequences are hashed to yield an integer identifier for the allele of each gene. Bacterial genomes are compared against each other by comparing the hash at each gene in the scheme, and a distance matrix is computed based on allele distances. explained examination we with an Adjusted Rand Index of 0.92. refMLST combines the advantages of SNV and gene-by-gene approaches to enable decentralized, reproducible bacterial typing without from each approach’s refMLST has been applied to of genomes across public health laboratories and is freely available for academic use at https://bugseq.com/academic. Abstract Having the ability to predict the protein-encoding gene content of a genome is of vital importance for a variety of bioinformatic tasks including binning genomes from metagenomic sequences, estimating genome completeness, and assessing risk due to the presence of antimicrobial resistance and other virulence genes. In this study, as a proof of concept, we built machine learning classifiers for predicting the presence or absence of the variable genes occurring in 10-90% of all publicly available high- quality E. coli genomes. The PATRIC genus-specific protein families were used to define orthologs across the set of genomes, and a single binary classifier was built for predicting the presence or absence of each family in each genome. Each model was built using the nucleotide k-mers from a set of 100 conserved genes as features. The resulting set of 3,259 XGBoost classifiers had an average macro F1 score of 0.907 ± 0.905-0.910 (± 95% confidence interval over 5 folds). Models predicting the presence or absence of genes that were well annotated, either occurring in subsystems, or having full EC numbers, had significantly higher F1 scores than those that did not, and models for genes with annotations involved with horizontal gene transfer, including those with the terms “plasmid” and “conjugation” had significantly lower F1 scores. We show that the F1 scores are stable across MLSTs, and that the trend can be recapitulated through sampling with a smaller numbers of core genes. Furthermore, we evaluate the extensibility of the models by classifying a holdout set of 422 diverse E. coli genomes that were isolated from freshwater environmental sources. Overall, this study provides a framework for predicting gene content from a limited amount input sequence data. Abstract Body: Background: Antimicrobial resistance (AMR) is a critical global health crisis, and methods to identify emerging AMR genes are essential to monitor and clinically manage its propagation. While homology is the primary line of evidence for predicting gene function, conserved gene order can also provide valuable information in instances where gene sequences diverge from known AMR conferring genes. Previous methods used for the classification of gene function have included gene order analysis, gene embeddings, and network representations of genomic data. However, these approaches in isolation may not integrate enough genomic context information to rule out false positives. Objective: In this work, we aim to train classifiers on gene embeddings to predict candidate divergent AMR genes. Here, an embedding is a vector representation of a given gene that encodes the occurrence of surrounding genes in its neighborhood across many genomes, allowing us to maximize the gene order context when making functional predictions. To generate such representations, we use gene order data in combination with G2Vec, a deep-learning method that uses network-based learning to construct gene embeddings and which has been successfully used to identify cancer prognostic genes. We posit that divergent AMR genes can be identified if they receive a high probability score from classifiers trained on the top gene embeddings of known AMR-conferring genes and are supported by the presence of other AMR genes in close proximity. Methods: We used G2Vec to obtain distributed gene representations for every gene present in the pangenomes of Enterococcus faecium, Escherichia coli, Salmonella enterica, Mycobacterium tuberculosis, and Klebsiella pneumoniae and identified the most representative embeddings for both the AMR-conferring and non-AMR-conferring sets. We then trained random forest, support vector machine, and XGBoost classifiers on these embeddings to predict probabilities for candidate AMR-conferring genes. Results: t-SNE visualizations of the top AMR-conferring and non-AMR-conferring representations for all species showed separable and tightly packed clusters, suggesting that they shared similar conservation characteristics. Our experimental results indicate that the optimized Random Forest classifier performed best, having achieved a 90.62% ROC-AUC score which signifies a strong ability to differentiate between the two sets. The trained model was used to predict the class probabilities of possible divergent AMR genes from all species and identify the top 10 highest supported candidates which were verified in terms of their functional annotations and top homologous protein hits against the Comprehensive Antibiotic Resistance Database (CARD). Conclusions: By incorporating existing knowledge about gene neighborhoods, the embedding-based approach of G2Vec was able to make accurate predictions in cases where the evidence from homology was less clear. Abstract Body: Genomic recombination in microbial populations is a critical generator of biological diversity, and it plays an important role in adaptation to new environments, hosts, and niches. Recombination detection has previously relied on alignment of genomic sequences and phylogenetic or comparative techniques that are extremely computationally expensive, especially with vast increases in av',\n",
       "   'venue': '',\n",
       "   'year': 2022,\n",
       "   'referenceCount': 0,\n",
       "   'citationCount': 0,\n",
       "   'influentialCitationCount': 0,\n",
       "   'isOpenAccess': False,\n",
       "   'openAccessPdf': None,\n",
       "   'fieldsOfStudy': None,\n",
       "   's2FieldsOfStudy': [{'category': 'Environmental Science',\n",
       "     'source': 's2-fos-model'},\n",
       "    {'category': 'Medicine', 'source': 's2-fos-model'},\n",
       "    {'category': 'Chemistry', 'source': 's2-fos-model'}],\n",
       "   'tldr': {'model': 'tldr@v2.0.0',\n",
       "    'text': 'A bioinformatic workflow called EDGE COVID-19 (EC-19) is developed that is capable of generating consensus genomes of SARS-CoV-2 and further perform pertinent downstream analyses and provides the ability to submit the consensus sequence and necessary metadata to GenBank, GISAID, andINSDC raw data repositories.'},\n",
       "   'publicationTypes': None,\n",
       "   'publicationDate': None,\n",
       "   'journal': None,\n",
       "   'citationStyles': {'bibtex': '@Inproceedings{None,\\n title = {Analytical Methods, and and wastewater data, recombinant analysis from SARS-CoV-2sequencing efforts.},\\n year = {2022}\\n}\\n'},\n",
       "   'authors': []}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rich_text': [{'text': {'content': 'a20411effeaac9aa457e528090dc274cb46c3412'}}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "properties.get('PaperId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'object': 'list', 'results': [{'object': 'page', 'id': 'e12b2d75-c70d-4f0f-bde1-00b24df946cd', 'created_time': '2024-08-13T09:36:00.000Z', 'last_edited_time': '2024-08-13T09:36:00.000Z', 'created_by': {'object': 'user', 'id': '6d558a8e-25e6-4afa-af8b-ae2dc29f5c2a'}, 'last_edited_by': {'object': 'user', 'id': '6d558a8e-25e6-4afa-af8b-ae2dc29f5c2a'}, 'cover': None, 'icon': None, 'parent': {'type': 'database_id', 'database_id': '3001b93d-fe62-442c-90c9-6bf6fa888feb'}, 'archived': False, 'in_trash': False, 'properties': {'FieldsOfStudy': {'id': '%3C_On', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': '[\"Medicine\"]', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '[\"Medicine\"]', 'href': None}]}, 'PublicationVenue': {'id': '%3DR_m', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': '{\"id\": \"099483df-e8f2-4bee-805d-8a69f07b6cbf\", \"name\": \"Nature Methods\", \"type\": \"journal\", \"alternate_names\": [\"Nat Method\"], \"issn\": \"1548-7091\", \"url\": \"http://www.nature.com/\", \"alternate_urls\": [\"http://www.nature.com/nmeth/index.html\", \"https://www.nature.com/nmeth/\", \"http://www.nature.com/nmeth/authors/index.html#aims\"]}', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '{\"id\": \"099483df-e8f2-4bee-805d-8a69f07b6cbf\", \"name\": \"Nature Methods\", \"type\": \"journal\", \"alternate_names\": [\"Nat Method\"], \"issn\": \"1548-7091\", \"url\": \"http://www.nature.com/\", \"alternate_urls\": [\"http://www.nature.com/nmeth/index.html\", \"https://www.nature.com/nmeth/\", \"http://www.nature.com/nmeth/authors/index.html#aims\"]}', 'href': None}]}, 'URL': {'id': 'BwcX', 'type': 'url', 'url': 'https://www.semanticscholar.org/paper/a20411effeaac9aa457e528090dc274cb46c3412'}, 'Authors': {'id': 'Fpvv', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'M. Mirdita, S. Ovchinnikov, Martin Steinegger', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'M. Mirdita, S. Ovchinnikov, Martin Steinegger', 'href': None}]}, 'ReferenceCount': {'id': 'JDdu', 'type': 'number', 'number': 50}, 'S2FieldsOfStudy': {'id': 'MTnc', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': '[{\"category\": \"Medicine\", \"source\": \"external\"}, {\"category\": \"Computer Science\", \"source\": \"s2-fos-model\"}, {\"category\": \"Biology\", \"source\": \"s2-fos-model\"}]', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '[{\"category\": \"Medicine\", \"source\": \"external\"}, {\"category\": \"Computer Science\", \"source\": \"s2-fos-model\"}, {\"category\": \"Biology\", \"source\": \"s2-fos-model\"}]', 'href': None}]}, 'PaperId': {'id': 'N%7Cbs', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'a20411effeaac9aa457e528090dc274cb46c3412', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'a20411effeaac9aa457e528090dc274cb46c3412', 'href': None}]}, 'Tldr': {'id': 'O%7Cu%5C', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ColabFold offers accelerated prediction of protein structures and complexes by combining the fast homology search of MMseqs2 with AlphaFold2 or RoseTTAFold, and becomes a free and accessible platform for protein folding.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ColabFold offers accelerated prediction of protein structures and complexes by combining the fast homology search of MMseqs2 with AlphaFold2 or RoseTTAFold, and becomes a free and accessible platform for protein folding.', 'href': None}]}, 'Venue': {'id': 'PvVR', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Nature Methods', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Nature Methods', 'href': None}]}, 'IsOpenAccess': {'id': 'WoHz', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'True', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'True', 'href': None}]}, 'PublicationDate': {'id': 'Zz%3Eb', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': '2022-05-30', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '2022-05-30', 'href': None}]}, 'OpenAccessPdf': {'id': '%5BdAN', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://www.nature.com/articles/s41592-022-01488-1.pdf', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://www.nature.com/articles/s41592-022-01488-1.pdf', 'href': None}]}, 'Abstract': {'id': 'bNI%7B', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'No abstract available.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'No abstract available.', 'href': None}]}, 'Year': {'id': 'i%5CK%3D', 'type': 'number', 'number': 2022}, 'PublicationTypes': {'id': 'jC%5E%40', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': '[\"JournalArticle\"]', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '[\"JournalArticle\"]', 'href': None}]}, 'Journal': {'id': 'qATL', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Nature Methods', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Nature Methods', 'href': None}]}, 'InfluentialCitationCount': {'id': 's%3ETz', 'type': 'number', 'number': 386}, 'ExternalIds': {'id': 'xEcb', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': '{\"PubMedCentral\": \"9184281\", \"DOI\": \"10.1038/s41592-022-01488-1\", \"CorpusId\": 237155730, \"PubMed\": \"35637307\"}', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '{\"PubMedCentral\": \"9184281\", \"DOI\": \"10.1038/s41592-022-01488-1\", \"CorpusId\": 237155730, \"PubMed\": \"35637307\"}', 'href': None}]}, 'CitationStyles': {'id': '%7CCDq', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': '{\"bibtex\": \"@Article{Mirdita2022ColabFoldMP,\\\\n author = {M. Mirdita and S. Ovchinnikov and Martin Steinegger},\\\\n booktitle = {Nature Methods},\\\\n journal = {Nature Methods},\\\\n pages = {679 - 682},\\\\n title = {ColabFold: making protein folding accessible to all},\\\\n volume = {19},\\\\n year = {2022}\\\\n}\\\\n\"}', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '{\"bibtex\": \"@Article{Mirdita2022ColabFoldMP,\\\\n author = {M. Mirdita and S. Ovchinnikov and Martin Steinegger},\\\\n booktitle = {Nature Methods},\\\\n journal = {Nature Methods},\\\\n pages = {679 - 682},\\\\n title = {ColabFold: making protein folding accessible to all},\\\\n volume = {19},\\\\n year = {2022}\\\\n}\\\\n\"}', 'href': None}]}, 'CitationCount': {'id': '~p%7C%3F', 'type': 'number', 'number': 3916}, 'Title': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'ColabFold: making protein folding accessible to all', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ColabFold: making protein folding accessible to all', 'href': None}]}}, 'url': 'https://www.notion.so/ColabFold-making-protein-folding-accessible-to-all-e12b2d75c70d4f0fbde100b24df946cd', 'public_url': None}], 'next_cursor': None, 'has_more': False, 'type': 'page_or_database', 'page_or_database': {}, 'request_id': 'e7f4c228-fb03-4d7c-b852-5de74d9175d8'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 定义过滤器和排序条件\n",
    "filter_criteria = {\"property\": \"PaperId\",\"rich_text\": {\"equals\": paper.get('paperId')}}\n",
    "# 查询数据库\n",
    "results = notion_client.query_database(NOTION_DATABASE_ID, filter_criteria=filter_criteria)\n",
    "\n",
    "# 处理结果\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total': 0, 'offset': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交互式文献搜索\n",
    "下面的代码实现了一个交互式的程序，用户可以通过输入查询关键字，动态获取并显示相关的学术论文结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d168c57647de48fb814e1fed1b60317d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='machine learning in atmospheric science', description='Query:', placeholder='Enter search query')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7bb334fe04a48f8b89d61d6241ec181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Search', icon='search', style=ButtonStyle(), tooltip='Search for papers')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec9aa31edc248a9a1e21de8cba5d358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ConnectTimeout",
     "evalue": "HTTPSConnectionPool(host='api.semanticscholar.org', port=443): Max retries exceeded with url: /graph/v1/paper/search?query=llava&offset=0&limit=20&fields=corpusId%2Curl%2Ctitle%2Cvenue%2CpublicationVenue%2Cyear%2Cauthors%2CexternalIds%2Cabstract%2CreferenceCount%2CcitationCount%2CinfluentialCitationCount%2CisOpenAccess%2CopenAccessPdf%2CfieldsOfStudy%2Cs2FieldsOfStudy%2CpublicationTypes%2CpublicationDate%2Cjournal%2CcitationStyles%2Ctldr&publicationTypes=Review%2CJournalArticle&minCitationCount=5 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001817ED359A0>, 'Connection to api.semanticscholar.org timed out. (connect timeout=None)'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32md:\\code\\.openscholar.venv\\Lib\\site-packages\\urllib3\\connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[1;32md:\\code\\.openscholar.venv\\Lib\\site-packages\\urllib3\\util\\connection.py:95\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetaddrinfo returns an empty list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\code\\.openscholar.venv\\Lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     84\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 85\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sock\n",
      "\u001b[1;31mTimeoutError\u001b[0m: [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectTimeoutError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32md:\\code\\.openscholar.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[1;32md:\\code\\.openscholar.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:404\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 404\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[1;32md:\\code\\.openscholar.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1058\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1058\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n",
      "File \u001b[1;32md:\\code\\.openscholar.venv\\Lib\\site-packages\\urllib3\\connection.py:363\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;66;03m# Add certificate verification\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    364\u001b[0m     hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[1;32md:\\code\\.openscholar.venv\\Lib\\site-packages\\urllib3\\connection.py:179\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout),\n\u001b[0;32m    183\u001b[0m     )\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mConnectTimeoutError\u001b[0m: (<urllib3.connection.HTTPSConnection object at 0x000001817ED359A0>, 'Connection to api.semanticscholar.org timed out. (connect timeout=None)')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32md:\\code\\.openscholar.venv\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32md:\\code\\.openscholar.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:799\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    797\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[1;32m--> 799\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32md:\\code\\.openscholar.venv\\Lib\\site-packages\\urllib3\\util\\retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[1;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[0;32m    594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='api.semanticscholar.org', port=443): Max retries exceeded with url: /graph/v1/paper/search?query=llava&offset=0&limit=20&fields=corpusId%2Curl%2Ctitle%2Cvenue%2CpublicationVenue%2Cyear%2Cauthors%2CexternalIds%2Cabstract%2CreferenceCount%2CcitationCount%2CinfluentialCitationCount%2CisOpenAccess%2CopenAccessPdf%2CfieldsOfStudy%2Cs2FieldsOfStudy%2CpublicationTypes%2CpublicationDate%2Cjournal%2CcitationStyles%2Ctldr&publicationTypes=Review%2CJournalArticle&minCitationCount=5 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001817ED359A0>, 'Connection to api.semanticscholar.org timed out. (connect timeout=None)'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectTimeout\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 40\u001b[0m, in \u001b[0;36msearch_papers\u001b[1;34m(b)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch_papers\u001b[39m(b):\n\u001b[0;32m     39\u001b[0m     query \u001b[38;5;241m=\u001b[39m query_input\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m---> 40\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43msemantic_scholar_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_papers_by_relevance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# 提取结果数据\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     data \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mD:\\code\\open_science_agent\\open_science_agent\\api\\semantic_scholar_client.py:39\u001b[0m, in \u001b[0;36mSemanticScholarClient.search_papers_by_relevance\u001b[1;34m(self, query, offset, limit)\u001b[0m\n\u001b[0;32m     28\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     29\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: query,\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffset\u001b[39m\u001b[38;5;124m\"\u001b[39m: offset,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminCitationCount\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m     38\u001b[0m }\n\u001b[1;32m---> 39\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBASE_URL\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/paper/search\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()  \u001b[38;5;66;03m# Raises an HTTPError if the request was unsuccessful\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32md:\\code\\.openscholar.venv\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\code\\.openscholar.venv\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\code\\.openscholar.venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32md:\\code\\.openscholar.venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32md:\\code\\.openscholar.venv\\Lib\\site-packages\\requests\\adapters.py:507\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[0;32m    505\u001b[0m     \u001b[38;5;66;03m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, NewConnectionError):\n\u001b[1;32m--> 507\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, ResponseError):\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RetryError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectTimeout\u001b[0m: HTTPSConnectionPool(host='api.semanticscholar.org', port=443): Max retries exceeded with url: /graph/v1/paper/search?query=llava&offset=0&limit=20&fields=corpusId%2Curl%2Ctitle%2Cvenue%2CpublicationVenue%2Cyear%2Cauthors%2CexternalIds%2Cabstract%2CreferenceCount%2CcitationCount%2CinfluentialCitationCount%2CisOpenAccess%2CopenAccessPdf%2CfieldsOfStudy%2Cs2FieldsOfStudy%2CpublicationTypes%2CpublicationDate%2Cjournal%2CcitationStyles%2Ctldr&publicationTypes=Review%2CJournalArticle&minCitationCount=5 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001817ED359A0>, 'Connection to api.semanticscholar.org timed out. (connect timeout=None)'))"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "\n",
    "# 导入SemanticScholarClient（假设它在open_science_agent包中）\n",
    "from open_science_agent.api.semantic_scholar_client import SemanticScholarClient\n",
    "\n",
    "semantic_scholar_client = SemanticScholarClient()\n",
    "\n",
    "# 定义查询输入框和按钮\n",
    "query_input = widgets.Text(\n",
    "    value='machine learning in atmospheric science',\n",
    "    placeholder='Enter search query',\n",
    "    description='Query:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "search_button = widgets.Button(\n",
    "    description='Search',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Search for papers',\n",
    "    icon='search'\n",
    ")\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "# 定义搜索函数\n",
    "def search_papers(b):\n",
    "    query = query_input.value\n",
    "    results = semantic_scholar_client.search_papers_by_relevance(query)\n",
    "    \n",
    "    # 提取结果数据\n",
    "    data = results['data']\n",
    "    papers = []\n",
    "    for paper in data:\n",
    "        papers.append({\n",
    "            'Title': paper['title'],\n",
    "            'Authors': ', '.join([author['name'] for author in paper['authors']]),\n",
    "            'Year': paper['year'],\n",
    "            'Venue': paper['venue'],\n",
    "            'Abstract': paper.get('abstract', 'No abstract available'),\n",
    "            'Citations': paper['citationCount'],\n",
    "            'URL': paper['url']\n",
    "        })\n",
    "    \n",
    "    # 转换为DataFrame\n",
    "    df = pd.DataFrame(papers)\n",
    "    \n",
    "    # 在输出框中显示结果\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "        display(df)\n",
    "\n",
    "# 绑定搜索按钮点击事件\n",
    "search_button.on_click(search_papers)\n",
    "\n",
    "# 显示查询输入框和按钮\n",
    "display(query_input, search_button, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f2ffa54dec4e5498688368797c401d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='machine learning in atmospheric science', description='查询:', placeholder='请输入查询关键字')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9521ac5441ef47dfaebec36aabebc6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='搜索', icon='search', style=ButtonStyle(), tooltip='搜索学术论文')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2870f4e4cbd415d936f3eb62fdbab40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed035ea8ba3c472dab02a2937f813249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "\n",
    "# 导入SemanticScholarClient（假设它在open_science_agent包中）\n",
    "from open_science_agent.api.semantic_scholar_client import SemanticScholarClient\n",
    "\n",
    "# 初始化SemanticScholarClient\n",
    "semantic_scholar_client = SemanticScholarClient()\n",
    "\n",
    "# 定义查询输入框和按钮\n",
    "query_input = widgets.Text(\n",
    "    value='machine learning in atmospheric science',\n",
    "    placeholder='请输入查询关键字',\n",
    "    description='查询:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "search_button = widgets.Button(\n",
    "    description='搜索',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='搜索学术论文',\n",
    "    icon='search'\n",
    ")\n",
    "\n",
    "output = widgets.Output()\n",
    "details_output = widgets.Output()\n",
    "\n",
    "# 定义显示详细信息的函数\n",
    "def show_paper_details(selected_paper):\n",
    "    with details_output:\n",
    "        clear_output(wait=True)\n",
    "        display(pd.DataFrame([selected_paper]))\n",
    "\n",
    "# 定义搜索函数\n",
    "def search_papers(b):\n",
    "    query = query_input.value\n",
    "    results = semantic_scholar_client.search_papers_by_relevance(query)\n",
    "    \n",
    "    # 提取结果数据\n",
    "    data = results['data']\n",
    "    papers = []\n",
    "    for paper in data:\n",
    "        papers.append({\n",
    "            '标题': paper['title'],\n",
    "            '作者': ', '.join([author['name'] for author in paper['authors']]),\n",
    "            '年份': paper['year'],\n",
    "            '期刊': paper['venue'],\n",
    "            '摘要': paper.get('abstract', '没有摘要'),\n",
    "            '引用数': paper['citationCount'],\n",
    "            '链接': paper['url'],\n",
    "            '详细信息': paper  # 添加整个论文数据，以便后续显示详细信息\n",
    "        })\n",
    "    \n",
    "    # 转换为DataFrame\n",
    "    df = pd.DataFrame(papers)\n",
    "    \n",
    "    # 创建交互表格\n",
    "    table = widgets.Output()\n",
    "    with table:\n",
    "        clear_output(wait=True)\n",
    "        display(df)\n",
    "    \n",
    "    # 创建选择器\n",
    "    dropdown = widgets.Dropdown(\n",
    "        options=[(paper['标题'], i) for i, paper in enumerate(papers)],\n",
    "        description='选择论文:',\n",
    "        disabled=False\n",
    "    )\n",
    "    \n",
    "    # 定义选择事件处理函数\n",
    "    def on_select(change):\n",
    "        selected_paper = papers[change['new']]['详细信息']\n",
    "        show_paper_details(selected_paper)\n",
    "    \n",
    "    dropdown.observe(on_select, names='value')\n",
    "    \n",
    "    # 显示查询结果和选择器\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "        display(table, dropdown)\n",
    "\n",
    "# 绑定搜索按钮点击事件\n",
    "search_button.on_click(search_papers)\n",
    "\n",
    "# 显示查询输入框和按钮\n",
    "display(query_input, search_button, output, details_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images found:\n",
      "\n",
      "Tables found:\n"
     ]
    }
   ],
   "source": [
    "import requests  \n",
    "from bs4 import BeautifulSoup  \n",
    "from urllib.parse import urljoin  \n",
    "\n",
    "def extract_images_and_tables(url):  \n",
    "    # 发送GET请求获取网页内容  \n",
    "    response = requests.get(url)  \n",
    "    response.raise_for_status()  # 检查请求是否成功  \n",
    "\n",
    "    # 使用BeautifulSoup解析网页内容  \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')  \n",
    "\n",
    "    # 提取所有图片的URL  \n",
    "    images = soup.find_all('img')  \n",
    "    image_urls = [urljoin(url, img['src']) for img in images if img.get('src')]  \n",
    "\n",
    "    # 提取所有表格  \n",
    "    tables = soup.find_all('table')  \n",
    "    table_htmls = [str(table) for table in tables]  \n",
    "\n",
    "    return image_urls, table_htmls  \n",
    "\n",
    "# 示例用法  \n",
    "url = 'https://www.semanticscholar.org/paper/a9eab711118774d1b897f70cbb0f420da144fe85'  # 替换为你要处理的URL  \n",
    "images, tables = extract_images_and_tables(url)  \n",
    "\n",
    "print(\"Images found:\")  \n",
    "for img in images:  \n",
    "    print(img)  \n",
    "\n",
    "print(\"\\nTables found:\")  \n",
    "for table in tables:  \n",
    "    print(table)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "def fetch_figures_and_tables(url):\n",
    "    # Fetch the webpage content\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Ensure the request was successful\n",
    "\n",
    "    # Parse the webpage content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Directory to save images\n",
    "    if not os.path.exists('images'):\n",
    "        os.makedirs('images')\n",
    "\n",
    "    # Extract figures and tables along with captions\n",
    "    figures_tables = []\n",
    "    \n",
    "    # Find all image objects that are associated with figures or tables\n",
    "    for img_tag in soup.find_all('img'):\n",
    "        # Check if it is a figure or table based on the caption or alt text\n",
    "        caption = img_tag.get('alt', 'No caption')\n",
    "        src = img_tag['src']\n",
    "\n",
    "        # Handle relative URLs by joining with the base URL\n",
    "        if not src.startswith('http'):\n",
    "            src = os.path.join(os.path.dirname(url), src)\n",
    "\n",
    "        # Download the image\n",
    "        img_data = requests.get(src).content\n",
    "        img_name = os.path.join('images', os.path.basename(src))\n",
    "        with open(img_name, 'wb') as handler:\n",
    "            handler.write(img_data)\n",
    "\n",
    "        figures_tables.append({'caption': caption, 'image': img_name})\n",
    "\n",
    "    return figures_tables\n",
    "\n",
    "# Example usage:\n",
    "url = 'https://www.semanticscholar.org/paper/A-Comprehensive-Review-of-AI-in-Healthcare%3A-Neural-Sathe-Deodhe/a9eab711118774d1b897f70cbb0f420da144fe85'\n",
    "figures_and_tables = fetch_figures_and_tables(url)\n",
    "\n",
    "# Print out the results\n",
    "for item in figures_and_tables:\n",
    "    print(f\"Caption: {item['caption']}\")\n",
    "    print(f\"Image File: {item['image']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".openscholar.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
